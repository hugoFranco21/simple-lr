import numpy  #numpy is used to make some operrations with arrays more easily
import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

__errors__= [];  #global variable to store the errors/loss for visualisation

columns = ['Job Title', 'Gender', 'Age', 'Performance Evaluation', 'Education', 'Department', 'Seniority', 'Base Pay', 'Bonus']

def prepare_data():
	df = pd.read_csv('datasets/glassdoor.csv', header=1, names = columns)
	return df

def h(params, sample):
	"""This evaluates a generic linear function h(x) with current parameters.  h stands for hypothesis
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		sample (lst) a list containing the values of a sample 
	Returns:
		Evaluation of h(x)
	"""
	acum = 0
	for i in range(len(params)):
		acum = acum + params[i]*sample[i]  #evaluates h(x) = a+bx1+cx2+ ... nxn.. 
	return acum


def show_errors(params, samples,y):
	"""Appends the errors/loss that are generated by the estimated values of h and the real value y
	
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		samples (lst) a 2 dimensional list containing the input samples 
		y (lst) a list containing the corresponding real result for each sample
	
	"""
	error_acum =0
#	print("transposed samples") 
#	print(samples)
	for i in range(len(samples)):
		hyp = h(params,samples[i])
		#print( "hyp  %f  y %f " % (hyp,  y[i]))   
		error=hyp-y[i]
		error_acum=+error**2 # this error is the original cost function, (the one used to make updates in GD is the derivated verssion of this formula)
	mean_error_param=error_acum/len(samples)
	__errors__.append(mean_error_param)

def GD(params, samples, y, alfa):
	"""
	Gradient Descent algorithm 
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		samples (lst) a 2 dimensional list containing the input samples 
		y (lst) a list containing the corresponding real result for each sample
		alfa(float) the learning rate
	Returns:
		temp(lst) a list with the new values for the parameters after 1 run of the sample set
	"""
	temp = list(params)
	general_error=0
	for j in range(len(params)):
		acum =0; error_acum=0
		for i in range(len(samples)):
			error = h(params,samples[i]) - y[i]
			acum = acum + error*samples[i][j]  #Sumatory part of the Gradient Descent formula for linear Regression.
		temp[j] = params[j] - alfa*(1/len(samples))*acum  #Subtraction of original parameter value with learning rate included.
	return temp

def main():
    df = prepare_data()
    df['TC'] = df['Bonus'] + df['Base Pay']
    df_y = df['TC']
    df_x = df[['Job Title', 'Gender', 'Performance Evaluation', 
    'Education', 'Department', 'Seniority']]
    one_hot_prof = pd.get_dummies(df['Job Title'])
    one_hot_dep = pd.get_dummies(df['Department'])
    one_hot_gender = pd.get_dummies(df['Gender'])
    one_hot_educ = pd.get_dummies(df['Education'])
    #print(one_hot_dep)
    #print(one_hot_prof)
    df_x = df_x.drop('Job Title', axis = 1)
    df_x = df_x.drop('Department', axis = 1)
    df_x = df_x.drop('Gender', axis = 1)
    df_x = df_x.drop('Education', axis = 1)
    #print(df_x.head())
    df_x = pd.concat([df_x, one_hot_gender, one_hot_educ, one_hot_prof], axis = 1)
    #print(df_x.head())
    print(df_x.columns)
    X_train = df_x.head(700).to_numpy()
    X_test = df_x.tail(299).to_numpy()
    y_train = df_y.head(700).to_numpy()
    y_test = df_y.tail(299).to_numpy()
    scaler = MinMaxScaler()
    data_minmax = scaler.fit_transform(X_train)
    print(data_minmax)
    params = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
    alfa = 0.01
    epochs = 0
    #print(data_minmax, 'X')
    #print(y_train, 'y')
    while True:  #  run gradient descent until local minima is reached
        oldparams = list(params)
        #print (params)
        params=GD(params, data_minmax,y_train,alfa)	
        show_errors(params, data_minmax, y_train)  #only used to show errors,it is not used in calculation
        #print (params)
        epochs = epochs + 1
        if(oldparams == params or epochs == 10):   #  local minima is found when there is no further improvement
            #print ("samples:")
            #print(data_minmax)
            #print ("final params:")
            #print (params)
            break
    y_pred = []
    norm_xtest = scaler.transform(X_test)
    for x in norm_xtest:
        aux = h(params, x)
        y_pred.append(aux)
        f = open("output/script2.txt", "w")

    f.write("Output of the script 2\n")
    f.write('\nscale ' + str(scaler.scale_))
    f.write('\nmin ' + str(scaler.min_))
	# The coefficients
    f.write('\nCoefficients: \n' + str(params))
# The mean squared error
    f.write('\nMean squared error: %.2f\n'
		% mean_squared_error(y_test, y_pred))
# The coefficient of determination: 1 is perfect prediction
    f.write('\nCoefficient of determination: %.2f\n'
		% r2_score(y_test, y_pred))
    f.close()

#use this to generate a graph of the errors/loss so we can see whats going on (diagnostics)

main()

plt.plot(__errors__)
plt.savefig('assets/error2.png')

plt.plot()